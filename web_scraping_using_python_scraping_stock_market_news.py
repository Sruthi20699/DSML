# -*- coding: utf-8 -*-
"""web scraping using python- scraping stock market news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a4X3Wg477uHmQW2Uvi2iJ5JEx1BbHCfQ

Web scraping - scraping stock market news


Web scraping is an automated method used to extract large amounts of data from websites. 
When you run the code for web scraping, a request is sent to the URL that you have mentioned. As a response to the request, the server sends the data and allows you to read the HTML or XML page. The code then, parses the HTML or XML page, finds the data and extracts it. 

To extract data using web scraping with python, you need to follow these basic steps:

    Find the URL that you want to scrape
    Inspecting the Page
    Find the data you want to extract
    Write the code
    Run the code and extract the data
    
 Algorithm
 
 step 1:Import the required libraries
     import requests:
         The requests module allows you to send HTTP requests using Python.
         The HTTP request returns a Response Object with all the response data (content, encoding, status, etc).
     import bs4:
     from bs4 import BeautifulSoup:
                 Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite    parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work
               
 Step 2: Inspecting the Page

The data is usually nested in tags. So, we inspect the page to see, under which tag the data we want to scrape is nested. To inspect the page, just right click on the element and click on “Inspect”.

Step 3: Find the data you want to extract

Step 4: Write the code

Step 5: Run the code and extract the data
"""

import requests
import bs4
from bs4 import BeautifulSoup

URL = "https://economictimes.indiatimes.com/markets/stocks/news"

headers = {
    'user - Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.56'
}

r = requests.get(URL,{'headers':headers})

soup=bs4.BeautifulSoup(r.text,'html.parser')

i=0
while i<10:
    print(i+1)
    print(soup.find_all('div',{'class':'eachStory'})[i].find_all('a')[-1].text)
    print(soup.find_all('div',{'class':'eachStory'})[i].find_all('p')[-1].text)
    i+=1